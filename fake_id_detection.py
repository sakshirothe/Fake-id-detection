# -*- coding: utf-8 -*-
"""Fake id detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4x_NmAx_LvEVuWQcFVzInR7EP5FpKT5
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load data
train = pd.read_csv("/content/train.csv")
test = pd.read_csv("/content/test.csv")

# Split into features and target
X = train.drop(columns=['fake'])
y = train['fake']

# Train-test split (for internal evaluation)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate model
y_pred = model.predict(X_val)
print("Accuracy on validation set:", accuracy_score(y_val, y_pred))
print("\nClassification Report:\n", classification_report(y_val, y_pred))

# Predict on full test set
X_test = test.drop(columns=['fake'])  # Drop existing 'fake' if present
test['predicted_fake'] = model.predict(X_test)

# Map prediction to labels
test['predicted_label'] = test['predicted_fake'].map({0: 'Real', 1: 'Fake'})

# Display first few predictions
print(test[['predicted_fake', 'predicted_label']].head())

# Optional: Save predictions to CSV
test.to_csv("instagram_fake_predictions.csv", index=False)

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Plotting the distribution of fake and genuine accounts
sns.countplot(x='fake', data=train)
plt.title("Distribution of Fake vs Genuine Accounts")
plt.show()

# Correlation matrix
correlation = train.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title("Feature Correlation Matrix")
plt.show()

sns.barplot(x='fake', y='profile pic', data=train)
plt.title("Profile Picture Presence in Fake vs Genuine Accounts")
plt.show()

sns.boxplot(x='fake', y='#followers', data=train)
plt.title("Followers Count in Fake vs Genuine Accounts")
plt.show()

sns.boxplot(x='fake', y='#follows', data=train)
plt.title("Following Count in Fake vs Genuine Accounts")
plt.show()

sns.boxplot(x='fake', y='#posts', data=train)
plt.title("Posts Count in Fake vs Genuine Accounts")
plt.show()

# Example: Feature Scaling (Optional)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_features = scaler.fit_transform(X_train)
scaled_data = pd.DataFrame(scaled_features,
columns=X_train.columns)
scaled_data['fake'] = y_train

# Split data into training and test sets

# Drop rows with NaN in the target variable before splitting
scaled_data_cleaned = scaled_data.dropna(subset=['fake'])

X = scaled_data_cleaned.drop('fake', axis=1)
y = scaled_data_cleaned['fake']
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.3, random_state=42)

# Build Random Forest Model
model = RandomForestClassifier(n_estimators=100,
random_state=42)
model.fit(X_train, y_train)

# Feature Importance Plot
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
sns.barplot(y=X.columns[indices], x=importances[indices],
palette='viridis')
plt.show()

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
from sklearn.metrics import confusion_matrix, classification_report
print("Classification Report:\n", classification_report(y_test,
y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_test, y_pred,
display_labels=['Genuine', 'Fake'], cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

# ðŸ“¦ 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
from sklearn.preprocessing import StandardScaler
import shap
import warnings
warnings.filterwarnings('ignore')

# ðŸ“¥ 2. Load Dataset
df = pd.read_csv("/content/train.csv")  # Replace with your actual file

# ðŸ§  3. Feature Engineering
# Assuming 'followers' and 'following' columns exist in your dataset
if 'followers' in df.columns and 'following' in df.columns:
  df['followers_to_following'] = df['followers'] / (df['following'] + 1)
else:
  df['followers_to_following'] = 0 # Or some other default/handling


# Assuming 'bio' column exists in your dataset
if 'bio' in df.columns:
  df['bio_length'] = df['bio'].fillna('').apply(len)
  df['has_link_in_bio'] = df['bio'].apply(lambda x: 1 if 'http' in str(x).lower() else 0)
else:
  df['bio_length'] = 0 # Or some other default/handling
  df['has_link_in_bio'] = 0 # Or some other default/handling


# ðŸ‘€ Check for nulls
df.fillna(0, inplace=True)

# ðŸ·ï¸ 4. Select Features
features = ['followers', 'following', 'posts', 'followers_to_following',
            'bio_length', 'has_link_in_bio', 'profile pic',
            'private'] # Updated features to match available data
target = 'fake'  # 0 = fake, 1 = real (Updated target variable name)

# Ensure all selected features exist in the DataFrame
features = [f for f in features if f in df.columns]
X = df[features]
y = df[target]


# ðŸ§ª 5. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# âš–ï¸ 6. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ðŸŒ² 7. Train Random Forest Model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# ðŸŽ¯ 8. Evaluate Model
y_pred = model.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ðŸ“Š 9. Feature Importance Plot
feat_imp = pd.Series(model.feature_importances_, index=features)
feat_imp.sort_values().plot(kind='barh', title='Feature Importance', figsize=(8,5))
plt.tight_layout()
plt.show()

# ðŸ” 10. SHAP Explainability
explainer = shap.Explainer(model, X_train_scaled)
shap_values = explainer(X_test_scaled[:100])  # Use a smaller sample for speed

shap.summary_plot(shap_values, X_test.iloc[:100], plot_type='bar')

df['username_length'] = df['nums/length username'].apply(len)
df['suspicious_username'] = df['nums/length username'].apply(lambda x: 1 if any(char.isdigit() for char in str(x)) and len(str(x)) > 15 else 0)
features.append('username_length')
features.append('suspicious_username')



# Engineer username-related features
# Ensure the 'username' column exists and handle potential missing values
if 'username' in original_train_df.columns:
  original_train_df['username'] = original_train_df['username'].fillna('') # Fill NaN with empty string

  df['username_length'] = original_train_df['username'].apply(len)
  df['suspicious_username'] = original_train_df['username'].apply(lambda x: 1 if any(char.isdigit() for char in str(x)) and len(str(x)) > 15 else 0)

  # Add the new features to the features list for modeling in cell pilKDDDy7ZXJ
  if 'username_length' not in features:
    features.append('username_length')
  if 'suspicious_username' not in features:
    features.append('suspicious_username')
else:
  print("Warning: 'username' column not found in the dataset. Skipping username feature engineering.")

df.to_csv("clean_fake_accounts.csv", index=False)
from google.colab import files
files.download("clean_fake_accounts.csv")

# Load the original training data to access username strings
original_train_df = pd.read_csv("/content/train.csv")